---
title: "Bayesian Network Environmental Risk Assessment of Pharmaceuticals"
output: html_notebook
---

Code for messing around with maps of Norway and WWTP by county.

# 1. Setup

```{r packages}
# Setup Renv for reproducible packages
# renv::init()
# renv::restore()
options(renv.download.override = utils::download.file)

# Library
library(tidyverse)
library(splmaps)
library(readxl)
library(glue)

# Leaflet stuff
library(splmaps)
options("rgdal_show_exportToProj4_warnings"="none")
library(sf)
library(leaflet)
library(rgdal)
library(shiny)

`%notin%` <- negate(`%in%`)

# Disable summarise informative message:
options(dplyr.summarise.inform = FALSE)
```

# 2. Data
Import Statistics Norway and Norwegian Institute for Public Health (NIPH) data and maps.
```{r data_import}
# Various Statistics Norway datasets
source(file = "src/data_importers/statistics_norway_import.R")
# SPL maps from the Norwegian Institute for Public Health
source(file = "src/data_importers/spl_map_import.R")

# Sales weights for 8 high-risk APIs, adapted from NIPH data
source(file = "src/data_importers/API_sales_import.R")
# Filter to the 6 APIs used in the Bayesian Network
analysed_APIs <- c("estradiol", "ethinylestradiol", "diclofenac", 
                      "ibuprofen", "paracetamol", "ciprofloxacin")
API_sales_weights_1999_2018 <- API_sales_weights_1999_2018 %>% 
  filter(API_Name %in% analysed_APIs)
# API PNECs from various sources
source(file = "src/data_importers/API_PNEC_import.R")

# Norwegian and English classifications of WWT technologies
source(file = "src/data_importers/WWT_class_import.R")

# van Dijk et al.'s unpublished pharmaceutical removal rates dataset
source(file = "src/data_importers/WWT_removal_import.R")
# match names using InChIKeys and filter results
API_removal_rates <- API_removal_rates %>% 
  left_join(API_sales_weights_1999_2018 %>% select(API_Name, InChIKey_string), 
            by = "InChIKey_string") %>% 
  distinct() %>% 
  mutate(API_Name = case_when(InChIKey_string == "mean" ~ "mean",
        TRUE ~ API_Name)) %>% 
  filter(!is.na(API_Name))
```


# 3. Methods

```{r data_processing_geographic}
# Summarise share of total population and WWTP class access by county (2020)
source(file = "src/data_processing/pop_share_county.R")
source(file = "src/data_processing/WWTP_share_county.R")

# Disc of WWTP share:
wwt_share_by_county_2020_disc <- wwt_share_by_county_2020 %>% 
    mutate(wwt_pop_share_disc = plyr::round_any(wwt_pop_share, 0.25, f = round))
# Check percentages = 100%
wwt_share_by_county_2020_disc %>% group_by(County_Name) %>% 
    summarise(sum(wwt_pop_share_disc))
# total for Norway
wwt_share_by_county_2020_disc %>% 
    group_by(Class_EU) %>% 
    summarise(sum(population)) %>% 
    summarise(`sum(population)` / sum(`sum(population)`))
# Add rows for total to the discretised dataframe

# Scenarios:
# Compliance: all secondary?
# Upgrade: all tertiary

# generate CPTs for no WWT (%), Primary, Secondary, Tertiary

# How many states should each node have? What values do these states have
WWT_level_states <- wwt_share_by_county_2020_disc %>% 
    group_by(Class_EU) %>% 
    summarise(n_states = n_distinct(wwt_pop_share_disc)) %>% 
    crossing(states = c(0, 0.25, 0.5, 0.75, 1)) %>% 
    group_by(Class_EU) %>% 
    # some intensely ugly code to produce a list of valid discretised states per treatment level
    summarise(n_in_class = row_number(),
              n_states,
              states) %>% 
    filter(n_in_class <= n_states) %>% 
    select(Class_EU, states) %>% 
    # no county has a tertiary level of 0.75, so we swap it for 1 (Oslo)
    mutate(states = case_when((Class_EU == "tertiary"& states == 0.75) ~ 1 ,
                              TRUE ~ states))

# Set up a big dataframe of all node values
all_removal_nodes <- wwt_share_by_county_2020_disc %>%
    select(County_Name, wwt_pop_share_disc, Class_EU) %>%
    # YAH: Make vector into a factor/preserve level order
    crossing(Scenario = factor(levels = c("Current", "Compliance", "Upgrade")),
             # use the WWT_level_states we set earlier to cross w/ relevant levels
             Value = WWT_level_states %>%
                 select(states) %>%
                 pull()) %>%
    mutate(present = as.numeric(wwt_pop_share_disc == Value)) %>% 
    arrange(Class_EU, County_Name, Scenario) %>% 
    pivot_wider(names_from = c("County_Name", "Scenario"), 
                values_from = present, 
                values_fill = 0) %>% 
    filter(wwt_pop_share_disc == Value)

test_CPT_dataframe <- all_removal_nodes %>% filter(Class_EU == "none") 


save_CPT <- function(CPT_dataframe, CPT_save_path){
    # Take the submitted dataframe
    # Remove columns used to format the CPT
    CPT_dataframe_selected <- CPT_dataframe %>% 
        select(-wwt_pop_share_disc, -Value, -Class_EU)

    # Values need decimal points or Hugin won't recognise them
    CPT_dataframe_selected <- CPT_dataframe_selected %>%
    mutate(across(everything(), as.character)) %>% 
    mutate(across(everything(), ~ paste0(., ".0")))  

    # Create a text file with HUGIN's preferred table header
    write_lines(x = "CPT", file = "data/Hugin/test_cpt_plz_ignore.txt")
    # Then add the CPT
    write.table(x = CPT_dataframe_selected, 
                file = "data/Hugin/test_cpt_plz_ignore.txt", 
                append = TRUE, col.names = FALSE, row.names = FALSE,
                sep = ",", quote = FALSE)
}

save_CPT(CPT_dataframe = test_CPT_dataframe,
         CPT_save_path = "data/Hugin/test_cpt_plz_ignore.txt")

```

Population discretisation was manually performed in Hugin, rounding population (in millions) up to the 
next highest 0.5. This is replicated here so we can check our choices of LM parameters.

```{r data_processing_population}
# Discretise population for diagnostic/summary graphs
source(file = "src/data_processing/pop_discretisation_Norway.R")
```


Calculate a linear model of Sales weight (kg), explained by population (mil) and year, plus
an intercept. This is used to predict sales weights under various population scenarios to 2050.

```{r data_processing_API}
# Fit linear models to sales data
source(file = "src/data_processing/API_sales_LM.R")
```

We can automate running through all possible combinations of APIs and scenarios in Hugin using
a data file (essentially a CSV) with scenario variables filled in, and specially formatted empty
columns for variables we want to pull out.

```{r data_processing_Hugin}
source(file = "src/data_processing/R_to_Hugin.R")
```

Now re-import data from Hugin

```{r data_import_Hugin}
source(file = "src/data_importers/Hugin_to_R_importer.R")
```

```{r data_processing_maps}
source(file = "src/data_processing/map_preparation.R")
```

# 6. Results



```{r shiny_map}
source(file = "src/interactive_graphics/shiny_data_viewer.R")
shinyApp(ui, server)

avg_RQ_county %>% 
    filter(API_Name == "Total",
           master_pop_scenario == "Main") %>% 
    group_by(master_year, County_Name) %>% 
    summarise(mean(Avg_RQ)) %>% 
    arrange(County_Name) %>% 
    ungroup() %>% 
    group_by(County_Name) %>% 
    summarise(perc_increase = max(`mean(Avg_RQ)`) / min(`mean(Avg_RQ)`))
```

```{r WWTP_bar}
source(file = "src/graphics/WWTP_RQ_bars.R")
WWTP_RQ_bars
```

```{r combined_risk_bar}
source(file = "src/graphics/combined_risk_bar.R")
```