---
title: "Bayesian Network Environmental Risk Assessment of Pharmaceuticals"
output: html_notebook
---

Code for messing around with maps of Norway and WWTP by county.

# 1. Setup

```{r packages}
# Setup Renv for reproducible packages
# renv::init()
# renv::restore()
options(renv.download.override = utils::download.file)

# Library
library(tidyverse)
library(splmaps)
library(readxl)
library(glue)

# Leaflet stuff
library(splmaps)
options("rgdal_show_exportToProj4_warnings"="none")
library(sf)
library(leaflet)
library(rgdal)
library(shiny)

`%notin%` <- negate(`%in%`)

# Disable summarise informative message:
options(dplyr.summarise.inform = FALSE)
```

# 2. Data
Import Statistics Norway and Norwegian Institute for Public Health (NIPH) data and maps.
```{r data_import}
# Various Statistics Norway datasets
source(file = "src/data_importers/statistics_norway_import.R")
# SPL maps from the Norwegian Institute for Public Health
source(file = "src/data_importers/spl_map_import.R")

# Sales weights for 8 high-risk APIs, adapted from NIPH data
source(file = "src/data_importers/API_sales_import.R")
# Filter to the 6 APIs used in the Bayesian Network
analysed_APIs <- c("estradiol", "ethinylestradiol", "diclofenac", 
                      "ibuprofen", "paracetamol", "ciprofloxacin")
API_sales_weights_1999_2018 <- API_sales_weights_1999_2018 %>% 
  filter(API_Name %in% analysed_APIs)
# API PNECs from various sources
source(file = "src/data_importers/API_PNEC_import.R")

# Norwegian and English classifications of WWT technologies
source(file = "src/data_importers/WWT_class_import.R")

# van Dijk et al.'s unpublished pharmaceutical removal rates dataset
source(file = "src/data_importers/WWT_removal_import.R")
# match names using InChIKeys and filter results
API_removal_rates <- API_removal_rates %>% 
  left_join(API_sales_weights_1999_2018 %>% select(API_Name, InChIKey_string), 
            by = "InChIKey_string") %>% 
  distinct() %>% 
  mutate(API_Name = case_when(InChIKey_string == "mean" ~ "mean",
        TRUE ~ API_Name)) %>% 
  filter(!is.na(API_Name))
```


# 3. Methods

```{r data_processing_geographic}
# Summarise share of total population and WWTP class access by county (2020)
source(file = "src/data_processing/pop_share_county.R")
source(file = "src/data_processing/WWTP_share_county.R")

# Disc of WWTP share:
wwt_share_by_county_2020_disc <- wwt_share_by_county_2020 %>% 
    mutate(wwt_pop_share_disc = plyr::round_any(wwt_pop_share, 0.25, f = round))
# Check percentages = 100%
wwt_share_by_county_2020_disc %>% group_by(County_Name) %>% 
    summarise(sum(wwt_pop_share_disc))
# total for Norway
wwt_share_by_county_2020_disc %>% 
    group_by(Class_EU) %>% 
    summarise(sum(population)) %>% 
    summarise(`sum(population)` / sum(`sum(population)`))

# Scenarios:
# Compliance: all secondary?
# Upgrade: all tertiary

# generate CPTs for no WWT (%), Primary, Secondary, Tertiary

# How many states should each node have? What values do these states have
WWT_level_states <- wwt_share_by_county_2020_disc %>% 
    group_by(Class_EU) %>% 
    summarise(n_states = n_distinct(wwt_pop_share_disc)) %>% 
    crossing(states = c(0, 0.25, 0.5, 0.75, 1)[n_states])
# This omits some in-between levels (e.g there's no 0.75 for tertiary, but it shouldn't be a problem?)

# Set up a big dataframe of all node values
temp <- wwt_share_by_county_2020_disc %>%
    select(County_Name, wwt_pop_share_disc, Class_EU) %>%
    # YAH: Make vector into a factor/preserve level order
    crossing(Scenario = factor(levels = c("Current", "Compliance", "Upgrade")),
             # use the WWT_level_states we set earlier to cross w/ relevant levels
             Value = WWT_level_states %>%
                 select(states) %>%
                 pull()) %>%
    mutate(present = as.numeric(wwt_pop_share_disc == Value)) %>% 
    arrange(Class_EU, County_Name, Scenario) %>% 
    pivot_wider(names_from = c("County_Name", "Scenario"), values_from = present)


for (v in 1:length(wwt_share_by_county_2020_disc$Class_EU %>% unique())) {
    temp_WWT_level <- (wwt_share_by_county_2020_disc$Class_EU %>% unique())[v]
    print(temp_WWT_level)
    temp_CPT_name <- glue("{temp_WWT_level}_WWT_CTP", temp_WWT_level)
    wwt_share_by_county_2020_disc %>% 
        select(County_Name, wwt_pop_share_disc, Class_EU) %>% 
        crossing(Scenario = c("Current", "Compliance", "Upgrade"),
                 # use the WWT_level_states we set earlier to cross w/ relevant levels
                 Value = WWT_level_states %>% 
                     filter(Class_EU == temp_WWT_level) %>% 
                     select(states) %>% 
                     pull()) %>% 
        mutate(present = as.numeric(wwt_pop_share_disc == Value))
        # Set levels if scenario = Compliance (all 2+) or Upgrade (all 3+)
        # mutate(present = case_when(Scenario == "Current" ~ as.numeric(wwt_pop_share_disc == Value),
        #                            Scenario == "Compliance" ~ ...,
        #                            Scenario == "Upgrade" ~ ...))

    
}
```

Population discretisation was manually performed in Hugin, rounding population (in millions) up to the 
next highest 0.5. This is replicated here so we can check our choices of LM parameters.

```{r data_processing_population}
# Discretise population for diagnostic/summary graphs
source(file = "src/data_processing/pop_discretisation_Norway.R")
```


Calculate a linear model of Sales weight (kg), explained by population (mil) and year, plus
an intercept. This is used to predict sales weights under various population scenarios to 2050.

```{r data_processing_API}
# Fit linear models to sales data
source(file = "src/data_processing/API_sales_LM.R")
```

We can automate running through all possible combinations of APIs and scenarios in Hugin using
a data file (essentially a CSV) with scenario variables filled in, and specially formatted empty
columns for variables we want to pull out.

```{r data_processing_Hugin}
source(file = "src/data_processing/R_to_Hugin.R")
```

Now re-import data from Hugin

```{r data_import_Hugin}
source(file = "src/data_importers/Hugin_to_R_importer.R")
```

```{r data_processing_maps}
source(file = "src/data_processing/map_preparation.R")
```

# 6. Results



```{r shiny_map}
source(file = "src/interactive_graphics/shiny_data_viewer.R")
shinyApp(ui, server)

avg_RQ_county %>% 
    filter(API_Name == "Total",
           master_pop_scenario == "Main") %>% 
    group_by(master_year, County_Name) %>% 
    summarise(mean(Avg_RQ)) %>% 
    arrange(County_Name) %>% 
    ungroup() %>% 
    group_by(County_Name) %>% 
    summarise(perc_increase = max(`mean(Avg_RQ)`) / min(`mean(Avg_RQ)`))
```

```{r WWTP_bar}
source(file = "src/graphics/WWTP_RQ_bars.R")
WWTP_RQ_bars
```

```{r combined_risk_bar}
source(file = "src/graphics/combined_risk_bar.R")
```