---
title: "Bayesian Network Environmental Risk Assessment of Pharmaceuticals"
output: html_notebook
---

Code for messing around with maps of Norway and WWTP by county.

# 1. Setup

```{r packages}
# Setup Renv for reproducible packages
# renv::init()
# renv::restore()
options(renv.download.override = utils::download.file)
# Update to a new version of R
# renv::upgrade() 
# renv::hydrate(update = "all") # populates the renv cache with copies of up to 
#                               # date package versions, needed by the project
#   # renv::update() should have no effect now, but running it as well won't harm 
#   # to check that all packages are indeed up to date
# renv::snapshot() # inspect the message before confirming to overwrite renv.lock

# Library
library(tidyverse)
library(splmaps)
library(readxl)
library(glue)
library(rgdal)
options("rgdal_show_exportToProj4_warnings"="none")
library(sf)
library(cowplot)

`%notin%` <- negate(`%in%`)

# Disable summarise informative message:
options(dplyr.summarise.inform = FALSE)
```

# 2. Data
Import Statistics Norway and Norwegian Institute for Public Health (NIPH) data and maps.
```{r data_import}
# Various Statistics Norway datasets
source(file = "src/data_importers/statistics_norway_import.R")
# SPL maps from the Norwegian Institute for Public Health
source(file = "src/data_importers/spl_map_import.R")

# Sales weights for 8 high-risk APIs, adapted from NIPH data
source(file = "src/data_importers/API_sales_import.R")
# Filter to the 6 APIs used in the Bayesian Network
analysed_APIs <- c("estradiol", "ethinylestradiol", "diclofenac", 
                      "ibuprofen", "paracetamol", "ciprofloxacin")
API_sales_weights_1999_2018 <- API_sales_weights_1999_2018 %>% 
  filter(API_Name %in% analysed_APIs)
# API PNECs from various sources
source(file = "src/data_importers/API_PNEC_import.R")

# van Dijk et al.'s unpublished pharmaceutical removal rates dataset
source(file = "src/data_importers/WWT_removal_import.R")
# match names using InChIKeys and filter results
API_removal_rates <- API_removal_rates %>% 
  left_join(API_sales_weights_1999_2018 %>% select(API_Name, InChIKey_string), 
            by = "InChIKey_string") %>% 
  distinct() %>% 
  mutate(API_Name = case_when(InChIKey_string == "mean" ~ "mean",
        TRUE ~ API_Name)) %>% 
  filter(!is.na(API_Name))
```


# 3. Methods

Population discretisation was manually performed in Hugin, rounding population (in millions) up to the 
next highest 0.5. This is replicated here so we can check our choices of LM parameters.

```{r data_processing_population}
# Discretise population for diagnostic/summary graphs
source(file = "src/data_processing/pop_discretisation_Norway.R")
```

```{r data_processing_maps}
# Set up a base map of Norway
source(file = "src/data_processing/map_preparation.R")
```

```{r map_of_counties}
library(ragg)
## Figure 02 ##
source(file = "src/graphics/WWTP_share_map.R")

Norway_county_graphic 

ggsave(filename = "output/images/ready/figure02_counties.png", device = agg_png,
       width = 30, height = 20, units = "cm", res = 300,
       scaling = 1.3) 

```

Calculate a linear model of Sales weight (kg), explained by population (mil) and year, plus
an intercept. This is used to predict sales weights under various population scenarios to 2050.

```{r data_processing_API}
## Figure 03 ##

# Fit linear models to sales data, and predict future sales from models
source(file = "src/data_processing/API_sales_LM.R")
# Graph out predictions from LMs
source(file = "src/graphics/sales_prediction_graph.R")
figure03_lm_graphs 

ggsave(filename = "output/images/ready/figure03_lm_graphs.png", device = agg_png, 
       width = 23, height = 12, units = "cm", dpi = 300,
       scaling = 1.2)

# Table 03
write_excel_csv(x = LM_parameters, file = "output/tables/tab_03_lm_params.csv")
```

We can automate running through all possible combinations of APIs and scenarios in Hugin using
a data file (essentially a CSV) with scenario variables filled in, and specially formatted empty
columns for variables we want to pull out.

```{r data_processing_Hugin}
source(file = "src/data_processing/R_to_Hugin.R")
```

Now re-import data from Hugin

```{r data_import_Hugin}
source(file = "src/data_importers/Hugin_to_R_importer.R")
# save a copy of data to output
write_excel_csv(x = Hugin_Data_Output_Tall_Labelled,
                file = "output/data/data_output.csv")

```


# 4. Results

```{r rq_by_year_growth}
## Figure 05 ##
source(file = "src/graphics/RQ_yeargrowth_bars.R")

RQ_Distribts_by_Year_Growth

# 7:8 Aspect Ratio to keep API names in full view
ggsave(filename = "output/images/ready/figure05_yeargrowth_RQ.png", device = "png", 
       width = 20, height = 20, units = "cm")

```

```{r WWTP_bar}
## Figure 06 ##
source(file = "src/graphics/WWTP_RQ_bars.R")
WWTP_RQ_bars

ggsave(filename = "output/images/ready/figure06_wwtp_scenario_RQ.png", device = "png", 
       width = 20, height = 20, units = "cm")
```
```{r combined_risk_bar}
## Figure 07 ##
source(file = "src/graphics/combined_vs_join_prob.R")
combined_join_comparison_bars
ggsave(filename = "output/images/ready/figure07_combined_vs_joint_RQ.png", device = "png", 
       width = 20, height = 20, units = "cm")
```