distinct() %>%
relocate(API_Name) %>%
pivot_longer(cols = 3:8,
names_to = "treatment_removal_rate")
# We'll reuse JvD's assumption that APIs without removal data will be removed
# at the average rate
API_removal_rates_mean <- API_removal_rates %>%
group_by(treatment_removal_rate) %>%
summarise(mean_removal = mean(value, na.rm = TRUE),
stdev_removal = sd(value, na.rm = TRUE))
### FK/FASS: API PNECs
API_PNECs <- read.csv(file = "Data/APIs_PBMT_byYear_2022-09-01_12.12.csv") %>%
select(API_Name, PNEC_gL) %>%
distinct()
NIPH_Sales_Weights_Summed <- NIPH_Sales_Weights_1999_2018 %>%
group_by(API_Name, Year) %>%
summarise(Total_Sold_kg = sum(API_AmountSoldValue, na.rm = TRUE) / 1000)
ggplot(data = NIPH_Sales_Weights_Summed,
mapping = aes(x = Year, y = Total_Sold_kg, colour = API_Name)) +
geom_line() +
scale_y_continuous(trans = "log10") +
labs(title = "Test Graph: Sales Weights 1999 - 2019 for Interesting APIs",
x = "Year",
y = "Total Sales, Log10 (kg)")
# Worth considering how sales weights are distributed, and how we might want to discretise them
ggplot(data = NIPH_Sales_Weights_Summed,
mapping = aes(x = Total_Sold_kg)) +
geom_histogram(bins = 100) +
scale_x_log10()
# Values vary from E+5 (paracetamol) to E-1 (ethinylestradiol)
NIPH_Sales_Weights_dif_quants <- NIPH_Sales_Weights_Summed %>%
group_by(API_Name) %>%
summarise(diff_1999_2018 = max(Total_Sold_kg) - min(Total_Sold_kg)) %>%
ungroup() %>%
summarise(quantile = quantile(diff_1999_2018, c(0.33, 0.66, 1)))
# Categorise APIs as light/medium/heavy based on which of the three quantiles they fall in
# NIPH_Sales_Weights_Summed <- NIPH_Sales_Weights_Summed %>%
#     mutate(quantile = case_when())
# YAH: A more pertinent way to discretise the API sales weights is probably actually to take the LMs and look at the variation there...
# For now, we'll eyeball it: Light (0-100 kg), Medium (100 - 10000 kg), Heavy (10000 - 1000000 kg)
NIPH_Sales_Weights_Summed <- NIPH_Sales_Weights_Summed %>%
mutate(weight_class = case_when(pmax(Total_Sold_kg) <= 100 ~ "light",
pmax(Total_Sold_kg) <= 10000 ~ "medium",
pmax(Total_Sold_kg) <= 10000000 ~ "heavy"))
# What share of the population does each county have?
total_pop_2020 <- pop_by_county_2020 %>%
summarise(sum(Population)) %>%
pull()
pop_share_by_county_2020 <- pop_by_county_2020 %>%
group_by(County_Code) %>%
summarise(County_Code,
Population,
Population_Share = round(Population / total_pop_2020, digits = 2))
# We can then use Population_Share to estimate sales weights per API, per county
API_sales_weight_by_county <- crossing(API_Sales, pop_share_by_county_2020) %>%
transmute(API_Name,
Year,
Vet,
County_Code,
AmountSold_County_g =  Population_Share * AmountSold_g)
# Or we can plug it straight in to the sales weights lm!
wwtp_by_county_2020 <- large_wwtp_by_county_2020 %>%
pivot_longer(cols = 1:7, names_to = "treatment", values_to = "population") %>%
mutate(size = "large") %>%
add_row(
small_wwtp_by_county_2020 %>%
pivot_longer(cols = 1:15, names_to = "treatment", values_to = "population") %>%
mutate(size = "small")
) %>%
left_join(WWTP_definitions %>% select(Name_EN, Class_EU), by = c("treatment" = "Name_EN"))
# How well do WWTP numbers and actual populations add up?
wwtp_by_county_2020 %>%
filter(treatment == "total") %>%
group_by(County_Name) %>%
summarise(sum(population), County_Code) %>%
distinct() %>%
left_join(y = pop_by_county_2020) %>%
summarise(`sum(population)` / Population)
# Pretty close, +- 5%, except for Troms & Finnmark, which is 15% higher than actual pop
# In any case, we can now characterise the proportions of different treatment levels in each County
wwt_share_by_county_2020 <-
wwtp_by_county_2020 %>%
filter(treatment != "total") %>%
group_by(County_Name, Class_EU) %>%
summarise(County_Name,
County_Code,
population = sum(population, na.rm = TRUE)) %>%
distinct() %>%
ungroup() %>%
group_by(County_Name) %>%
mutate(wwt_pop_share = round(population / sum(population), digits = 2))
ggplot(data = wwt_share_by_county_2020, aes(x = wwt_pop_share, fill = Class_EU)) +
geom_histogram(bins = 5)
# For discretisation of WWT nodes it'd be nice to get the range of each treatment level
wwt_share_by_county_2020_disc <-
wwt_share_by_county_2020 %>%
# Discretise to 5 states (0 - 100%), rounding to nearest 25%
mutate(wwt_pop_share_disc = plyr::round_any(wwt_pop_share, 0.25, f = round))
Norway_Pop_Discretisation <- Norway_Population_Year %>%
transmute(Year,
Pop_mil = Population / 1e6,
Scenario = "Historic") %>%
add_row(Norway_Population_Projections_21C %>% transmute(Year,
Pop_mil = Population / 1e6,
Scenario)) %>%
filter(Year <= 2050) %>%
# Round to the highest 0.5 pop
mutate(Pop_mil_disc = plyr::round_any(Pop_mil, 0.5, f = ceiling))
ggplot(data = Norway_Pop_Discretisation %>% filter(Year >= 2000),
mapping = aes(x = Year,
y = Pop_mil,
colour = Scenario)) +
geom_point() +
geom_line(aes(y = Pop_mil_disc)) +
scale_colour_discrete(limits = c("Historic",
"Low national growth (LLL)",
"Main alternative (MMM)",
"High national growth (HHH)"))
API_sales_by_population <-
NIPH_Sales_Weights_Summed %>% left_join(y = Norway_Population_Year, by = c("Year"))
# Linear Model: API ~ Population + Year
LMs_API <- API_sales_by_population %>%
mutate(Population_mil = Population / 1e6) %>%
filter(Year != 2019) %>%
group_by(API_Name) %>%
summarise(lm_intercept_kg = coef(lm(Total_Sold_kg/Population_mil ~ Year))[[1]],
lm_coef_kg_per_mperson_per_year = coef(lm(Total_Sold_kg/Population_mil ~ Year))[[2]],
weight_class) %>%
distinct() %>%
# Pop PNECs on the end so we have to import fewer dfs to python
left_join(API_PNECs, by = "API_Name")
# Recalculate sales weights from the LM for comparison
Sales_Projections_21C <-
crossing(LMs_API, Norway_Pop_Discretisation) %>%
mutate(Sales_Proj_kg = (Year * lm_coef_kg_per_mperson_per_year + lm_intercept_kg)
* Pop_mil_disc) %>%
filter(Year %in% c(2010, 2020, 2030, 2040, 2050))
# Join projections and records together to plot on one graph
Sales_Projections_Records <- Sales_Projections_21C %>%
transmute(API_Name,
Total_Sold_kg = Sales_Proj_kg,
Scenario,
Year) %>%
add_row(NIPH_Sales_Weights_Summed %>% transmute(API_Name,
Year,
Total_Sold_kg,
Scenario = "Measured")) %>%
filter(Year != 2019)
ggplot(data = Sales_Projections_Records %>% filter(Scenario != "Measured", API_Name != "diclofenac"),
mapping = aes(x = Year,
y = pmax(0, Total_Sold_kg),
colour = Scenario,
shape = Scenario)) +
geom_point(size = 1, stroke = 1.3) +
geom_point(data = Sales_Projections_Records %>% filter(Scenario == "Measured", API_Name != "diclofenac"),
colour = "black", size = 1) +
scale_shape_manual(values = c("Measured" = 16,
"Historic" = 4,
"High national growth (HHH)" = 4,
"Main alternative (MMM)" = 4,
"Low national growth (LLL)" = 4)) +
scale_colour_discrete(limits = c("Measured",
"Historic",
"High national growth (HHH)",
"Main alternative (MMM)",
"Low national growth (LLL)")) +
scale_y_continuous(limits = c(0, NA)) +
facet_wrap(facets = vars(API_Name), scales = "free") +
labs(x = "Year", y = "Total Sold (kg)")
# Make data files for automated data input/output to Hugin
Six_Interesting_APIs <- Interesting_APIs[c(1, 2, 4:7)]
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", str_to_title(Six_Interesting_APIs)),
c("SumRQ_Estrogens", "SumRQ_Antibiotics",
"SumRQ_Painkillers","SumRQ_Total"))
All_RQ_Boolean_Nodes <- c("PRQ_1_Estrogens", "PRQ_1_Antibiotics", "PRQ_1_Painkillers", "PRQ_1_Total")
Hugin_Data_File <- tibble(master_pop_scenario = as_factor(c("Low", "Main", "High"))) %>%
# Use vector recycling via crossing to set up the various scenario combinations easily
crossing(master_year = c("2020", "2050"),
master_WWT_scenario = c("Current", "Compliance"),
master_county = unlist(county_codes[2])) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# renv::init()
renv::restore()
# Library
library(tidyverse)
library(splmaps)
library(readxl)
library(glue)
`%notin%` <- negate(`%in%`)
# Disable summarise informative message:
options(dplyr.summarise.inform = FALSE)
# Make data files for automated data input/output to Hugin
Six_Interesting_APIs <- Interesting_APIs[c(1, 2, 4:7)]
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", str_to_title(Six_Interesting_APIs)),
c("SumRQ_Estrogens", "SumRQ_Antibiotics",
"SumRQ_Painkillers","SumRQ_Total"))
All_RQ_Boolean_Nodes <- c("PRQ_1_Estrogens", "PRQ_1_Antibiotics", "PRQ_1_Painkillers", "PRQ_1_Total")
Hugin_Data_File <- tibble(master_pop_scenario = as_factor(c("Low", "Main", "High"))) %>%
# Use vector recycling via crossing to set up the various scenario combinations easily
crossing(master_year = c("2020", "2050"),
master_WWT_scenario = c("Current", "Compliance"),
master_county = unlist(county_codes[2])) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# rename_with(cols = 8:15, ~str_remove(string = ., pattern = "\\[MEAN\\]")) %>%
# rename_with(cols = 8:15, ~str_remove_all(string = ., pattern = "\\W"))
# Need to refactorise population scenarios
mutate(master_pop_scenario = fct_relevel(master_pop_scenario, c("Low", "Main", "High")),
master_WWT_scenario = as.ordered(master_WWT_scenario)) %>%
select(-c(5:10))
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 5:72,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+")) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
# I'm gonna make some graphs!
# Sales Weight, by year, scenario & API
# SW_by_Year_Scen_API <- ggplot(data = Hugin_Data_Output,
#        mapping = aes(x = Year,
#                      y = `[MEAN](API_Sales_Weight_kg)`,
#                      colour = Population_Scenario,
#                      shape = Population_Scenario)) +
#   geom_point() +
#   geom_path() +
#   facet_wrap(facets = vars(API_Name), ncol = 4, scales = "free") +
#   scale_color_discrete(breaks = c("Low", "Main", "High")) +
#   scale_shape_discrete(breaks = c("Low", "Main", "High"))
#
# SW_by_Year_Scen_API
# What's going on with these APIs?
# Diclofenac_Weird <- Hugin_Data_Output %>%
#   filter(API_Name == "diclofenac",
#          WWTP_Removal_Scenario == "0 - 0.001") %>%
#   select(API_Name, Year, `[MEAN](API_Sales_Weight_kg)`, Population_Scenario) %>%
#   distinct()
# Append county names to the SPL map
Norway_county_map_names <- Norway_counties_shapefile %>%
left_join(y = county_codes, by = "County_Code") %>%
left_join(pop_by_county_2020, by = "County_Code")
# Find the centroids of counties for better labelling
Norway_county_map_centroids <- Norway_county_map_names %>%
group_by(County_Name) %>%
summarise(lat = mean(range(lat)),
long = mean(range(long)))
Norway_county_map <- ggplot(data = Norway_county_map_names, mapping = aes(x = long,
y = lat)) +
geom_polygon(color = "grey",
size = 0.3,
aes(group = group)) +
theme_void()
# Cities of 50,000+ only
Norway_Cities %>% filter(population > 50000) %>%
summarise(sum(population))
# This covers only 2.1 million people, not even half of the population
# Removing the filter covers 4.3 million people, which is better...
Norway_county_cities_map <- ggplot(data = Norway_county_map_names, mapping = aes(x = long,
y = lat)) +
geom_polygon(color = "grey",
size = 0.1,
aes(group = group,
fill = Population)) +
scale_fill_distiller(type = "seq",
direction = 1,
palette = "Greys") +
geom_text_repel(data = Norway_county_map_centroids,
size = 4,
alpha = 0.5,
mapping = aes(label = County_Name),) +
geom_point(data = Norway_Cities %>% filter(population > 50000),
alpha = 1,
colour = "red",
aes(size = population)) +
geom_label_repel(data = Norway_Cities %>% filter(population > 50000),
mapping = aes(label = city))
# API_Sum_RQ <- Hugin_Data_Output_Tall %>%
#     filter(API_Name == "Total", Risk_Type == "SumRQ") %>%
#     rename(County_Name = master_county) %>%
#     left_join(Norway_county_map_names) %>%
#     filter( master_WWT_scenario == "Current") %>%
#     pivot_wider(names_from = "Risk_Bin", values_from = "Probability")
#
# API_Sum_RQ_Centroids <- Norway_county_map_centroids %>%
#     left_join(Hugin_Data_Output_Tall, by = c("County_Name" = "master_county")) %>%
#     filter( master_WWT_scenario == "Current") %>%
#     filter(API_Name == "Total", Risk_Type == "SumRQ") %>%
#     filter(master_year== 2020, master_WWT_scenario == "Current")
# # Make a barplot of risk
# test_bar_plot.list <- lapply(1:length(API_Sum_RQ_Centroids), function(i)) {
#     # for
#
#
#
# }
#
#     ggplot(data = API_Sum_RQ_Centroids %>%
#                                         filter(County_Name == "Agder", master_pop_scenario == "Main"),
#        mapping = aes(x = Risk_Bin, y = Probability, fill = Risk_Bin)) +
#     geom_col() +
#     theme_void() +
#     scale_fill_viridis_d() +
#     theme(legend.position = "none",
#           panel.background = element_rect(fill='transparent'),
#           plot.background = element_rect(fill='transparent', color=NA),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           legend.background = element_rect(fill='transparent'),
#           legend.box.background = element_rect(fill='transparent'))
#
# test_bar_plot_grob <- as.grob(test_bar_plot)
# Create a tibble to contain the barplots
test_bar_plot_tibble = tibble(y = 58.82622, x = 7.87337, grob = list(test_bar_plot_grob))
API_Exceedence_Risk <- Hugin_Data_Output_Tall %>%
filter(API_Name == "Total", Risk_Type == "PRQ_1", master_WWT_scenario == "Current", Risk_Bin == "true") %>%
rename(County_Name = master_county) %>%
left_join(Norway_county_map_names) %>%
pivot_wider(names_from = "Risk_Bin", values_from = "Probability")
Norway_Exceedence_Map <- ggplot(data = API_Exceedence_Risk,
mapping = aes(x = long,
y = lat,
fill = true)) +
geom_polygon(color = "white",
size = 0.3,
aes(group = group)) +
facet_grid(rows = vars(master_year), cols = vars(master_pop_scenario))
Norway_Exceedence_Map
sum_RQ_WWTP_data <- Hugin_Data_Output_Tall %>% filter(Risk_Type != "PRQ_1",
API_Name == "Total",
master_county == "Agder") %>%
mutate(Risk_Bin = as.ordered(Risk_Bin),
master_WWT_scenario = fct_relevel(master_WWT_scenario, c("Current", "Compliance")))
ggplot(data = sum_RQ_WWTP_data,
mapping = aes(x = master_WWT_scenario,
y = Probability,
fill = Risk_Bin)) +
geom_col(position = "stack") +
scale_fill_viridis_d() +
facet_grid(cols = vars(master_year))
# Now we're gonna try and make some graphs like Sophie's!
for (i in 1:length(Interesting_APIs)) {
API_Name <- Interesting_APIs[i]
RQ_Binned_Hugin_Output <- Hugin_Data_Output %>%
filter(API_Name == Interesting_APIs[i]) %>%
select(1:4, 16:21) %>%
pivot_longer(cols = 5:10,
names_to = "RQ_Bin",
values_to = "Probability")
RQ_by_Bin_Graph <- ggplot(data = RQ_Binned_Hugin_Output,
mapping = aes(x = Year,
y = Probability,
fill = RQ_Bin)) +
geom_col(position = position_fill(reverse = TRUE)) +
facet_grid(rows = vars(Population_Scenario),
cols = vars(WWTP_Scenario)) +
scale_fill_viridis_d(option = "inferno", direction = 1) +
labs(title = Interesting_APIs[i])
assign(paste("RQ_by_Bin_Graph", Interesting_APIs[i], sep = "_"), RQ_by_Bin_Graph)
}
RQ_by_Bin_Graph_amoxicillin
library(sf)
library(leaflet)
library(shiny)
library(tidyverse)
library(rgdal)
# Set up data from main RMD
pd_county <- splmaps::nor_nuts3_map_b2020_default_sf %>%
mutate(County_Code = str_extract(location_code, pattern = "[0-9]{2}")) %>%
left_join(county_codes, by = "County_Code") %>%
select(-location_code)
avg_RQ_county <- Hugin_Data_Output_Tall %>%
group_by(master_pop_scenario, master_year, master_WWT_scenario, master_county, API_Name) %>%
mutate(Risk_Bin_avg = case_when(Risk_Bin == "0-1" ~ 0.5,
Risk_Bin == "1 - 10" ~ 5.5,
Risk_Bin == "10-100" ~ 55,
Risk_Bin == "100-1000" ~ 550,
Risk_Bin == "1000-10000" ~ 5500,
Risk_Bin == "10000-inf" ~ 10000),
County_Name = master_county) %>%
summarise(Avg_RQ = sum(Risk_Bin_avg * Probability, na.rm = TRUE),
County_Name,
API_Name) %>%
ungroup() %>%
select(-master_county) %>%
distinct()
pd_county_joined <- merge(pd_county, avg_RQ_county)
### SHINY / LEAFLET ###
## UI ##
ui <- fluidPage(titlePanel("Average Sum of Pharmaceutical Risk Quotients, Norway"),
sidebarPanel(
radioButtons(inputId = "radio_pop_scen",
h3("Population Growth Scenario"),
choices = list("Low", "Main", "High"),
selected = "Main"),
radioButtons(inputId = "radio_wwt_scen",
h3("WWT Scenario"),
choices = list("Current", "Compliance"),
selected = "Current"),
sliderInput(inputId = "slider_year",
h3("Year"),
min = 2020, max = 2050, value = 2020, step = 30, sep = ""),
selectInput(inputId = "select_API_name",
h3("API or Group"),
choices = c("Estradiol", "Ethinylestradiol", "Diclofenac",
"Ibuprofen", "Paracetamol", "Ciprofloxacin",
"Estrogens", "Antibiotics", "Painkillers", "Total"),
selected = "Total")
),
mainPanel(
# Add a map to the UI
leafletOutput("test_map")
)
)
## SERVER ##
server <- function(input, output) {
# Create a palette for average Sum RQs
AvgRQ_Pal <- colorNumeric(palette = "viridis", domain = c(0, 6000))
# Reactively filter pd_county_joined to the selected year and scenarios
pd_county_joined_filtered <- reactive({
req(input$slider_year)
pd_county_joined %>% filter(master_year == input$slider_year,
master_pop_scenario == input$radio_pop_scen,
master_WWT_scenario == input$radio_wwt_scen,
API_Name == input$select_API_name)
})
output$test_map <- renderLeaflet({
leaflet(data = pd_county_joined_filtered()) %>%
addProviderTiles("Stamen.TonerLite") %>%
addPolygons(data = pd_county_joined_filtered(),
weight = 0.3,
opacity = 1,
fillColor = ~AvgRQ_Pal(Avg_RQ),
layerId = ~County_Name,
color = "white",
fillOpacity = 0.5,
highlightOptions = highlightOptions(
weight = 1,
color = "#666",
# If dashArray = "". as in the vignette, only the first polygon will appear
dashArray = NULL,
fillOpacity = 0.7,
bringToFront = TRUE),
label = sprintf(
"<strong>%s</strong><br/>Mean RQ = %g",
pd_county_joined_filtered()$County_Name,
pd_county_joined_filtered()$Avg_RQ) %>%
lapply(htmltools::HTML),
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
direction = "auto")
) %>%
addLegend(position = "bottomright",
pal = AvgRQ_Pal,
values = c(0, 6000),
opacity = 1,
title = "Mean RQ",
)
})
}
shinyApp(ui = ui, server = server)
