#   # renv::update() should have no effect now, but running it as well won't harm
#   # to check that all packages are indeed up to date
# renv::snapshot() # inspect the message before confirming to overwrite renv.lock
# Library
library(tidyverse)
library(splmaps)
library(readxl)
library(glue)
# Leaflet stuff
library(splmaps)
library(rgdal)
options("rgdal_show_exportToProj4_warnings"="none")
library(sf)
library(leaflet)
library(shiny)
library(svglite)
library(scatterpie)
library(scico)
library(cowplot)
`%notin%` <- negate(`%in%`)
# Disable summarise informative message:
options(dplyr.summarise.inform = FALSE)
# Various Statistics Norway datasets
source(file = "src/data_importers/statistics_norway_import.R")
# SPL maps from the Norwegian Institute for Public Health
source(file = "src/data_importers/spl_map_import.R")
# Sales weights for 8 high-risk APIs, adapted from NIPH data
source(file = "src/data_importers/API_sales_import.R")
# Filter to the 6 APIs used in the Bayesian Network
analysed_APIs <- c("estradiol", "ethinylestradiol", "diclofenac",
"ibuprofen", "paracetamol", "ciprofloxacin")
API_sales_weights_1999_2018 <- API_sales_weights_1999_2018 %>%
filter(API_Name %in% analysed_APIs)
# API PNECs from various sources
source(file = "src/data_importers/API_PNEC_import.R")
# # Norwegian and English classifications of WWT technologies
# source(file = "src/data_importers/WWT_class_import.R")
# van Dijk et al.'s unpublished pharmaceutical removal rates dataset
source(file = "src/data_importers/WWT_removal_import.R")
# match names using InChIKeys and filter results
API_removal_rates <- API_removal_rates %>%
left_join(API_sales_weights_1999_2018 %>% select(API_Name, InChIKey_string),
by = "InChIKey_string") %>%
distinct() %>%
mutate(API_Name = case_when(InChIKey_string == "mean" ~ "mean",
TRUE ~ API_Name)) %>%
filter(!is.na(API_Name))
# Summarise share of total population by county (2020)
source(file = "src/data_processing/pop_share_county.R")
# Discretise population for diagnostic/summary graphs
source(file = "src/data_processing/pop_discretisation_Norway.R")
# Set up a base map of Norway
source(file = "src/data_processing/map_preparation.R")
source(file = "src/graphics/WWTP_share_map.R")
# Norway_county_WWT_map
# ggsave(filename = "output/images/raw/figure02_map_WWT.svg", device = "svg",
#        width = 5, height = 10, units = "cm")
#
# Norway_county_pies_WWT
# ggsave(filename = "output/images/raw/figure02_pies_WWT.svg", device = "svg",
#        width = 20, height = 10, units = "cm")
#
# API_removal_rates_bars
# ggsave(filename = "output/images/raw/figure02_bars_removal.svg", device = "svg",
#        width = 20, height = 5, units = "cm")
# YAH 15:14 - 13/01/2023 - changed the year value in the lm to n - 2000, now
# need to tweak the rest accordingly
# Fit linear models to sales data, and predict future sales from models
source(file = "src/data_processing/API_sales_LM.R")
# Graph out predictions from LMs
source(file = "src/graphics/sales_prediction_graph.R")
figure03_lm_graphs +
theme_minimal()
ggsave(filename = "output/images/raw/figure03_lm_graphs.svg", device = "svg",
width = 23, height = 12, units = "cm")
# This turned out to be hard to do automatically, so I did it by hand.
# source(file = "src/data_exporters/WWT_share_CPT_exporter.R")
source(file = "src/data_processing/R_to_Hugin.R")
source(file = "src/data_importers/Hugin_to_R_importer.R")
viridis_colour <- c("viridis", "magma", "plasma", "inferno")[4]
# Set a consistent mapping of RQs to scale values
viridis_RQ_mapping <- Hugin_Data_Output_Tall %>%
select(Risk_Bin) %>%
filter(Risk_Bin %notin% c("true", "false")) %>%
distinct() %>%
pull()
API_labeller <- str_to_title(pull(LMs_API[1]))
names(API_labeller) <- API_labeller
API_labeller <- c(API_labeller, AllAPI = "All APIs")
county_labels <- as_labeller(c(Viken = "Viken (Urban)",
Nordland = "Nordland (Rural)",
Oslo = "Oslo (Urban)",
`Whole Country` = "Whole Country",
API_labeller))
source(file = "src/graphics/RQ_map_year.R")
# Save as SVG for modification
Norway_county_pies_RQ
ggsave(filename = "output/images/raw/figure04a_pies_RQ.svg", device = "svg",
width = 20, height = 10, units = "cm")
Norway_county_RQ_map
ggsave(filename = "output/images/raw/figure04b_map_RQ.svg", device = "svg",
width = 20, height = 10, units = "cm")
Norway_county_bars_RQ <- Norway_county_bars_RQ +
theme(legend.position = "none")
plot_grid(Norway_county_RQ_map, Norway_county_bars_RQ,
align = "v",
nrow = 2, rel_heights = c(2, 1))
ggsave(filename = "output/images/raw/figure04bc_map_bars_RQ.svg", device = "svg",
width = 25, height = 15, units = "cm")
source(file = "src/graphics/WWTP_RQ_bars.R")
WWTP_RQ_bars
ggsave(filename = "output/images/ready/figure05_wwtp_scenario_RQ.png", device = "png",
width = 20, height = 10, units = "cm")
source(file = "src/graphics/pop_scen_RQ_bars.R")
pop_scen_RQ_bars
ggsave(filename = "output/images/ready/figure06_pop_scenario_RQ.png", device = "png",
width = 20, height = 10, units = "cm")
source(file = "src/graphics/combined_risk_bar.R")
combined_RQ_bars
ggsave(filename = "output/images/ready/figure07_combined_RQ.png", device = "png",
width = 23, height = 15, units = "cm")
# Make data files for automated data input/output to Hugin
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", str_to_title(analysed_APIs)),
c("RQ_FW_Estrogens", "RQ_FW_Painkillers","RQ_FW_AllAPI"))
All_RQ_Boolean_Nodes <- c("PRQ_GT_Estrogens", "PRQ_GT_Painkillers", "PRQ_GT_AllAPI")
Hugin_Data_File <- tibble(scenario_number = 1:36) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
# print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
# print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
View(Hugin_Data_File)
# Make data files for automated data input/output to Hugin
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", str_to_title(analysed_APIs)),
c("RQ_FW_Estrogens", "RQ_FW_Painkillers","RQ_FW_AllAPI"))
All_RQ_Boolean_Nodes <- c("PRQ_GT_Estrogens", "PRQ_GT_Painkillers", "PRQ_GT_AllAPI")
Hugin_Data_File <- tibble(scenario_number = 1:36) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v] %>% tolower()
# print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
# print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
All_RQ_Interval_Nodes
# Make data files for automated data input/output to Hugin
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", tolower(analysed_APIs)),
c("RQ_FW_Estrogens", "RQ_FW_Painkillers","RQ_FW_AllAPI"))
All_RQ_Boolean_Nodes <- c("PRQ_GT_Estrogens", "PRQ_GT_Painkillers", "PRQ_GT_AllAPI")
Hugin_Data_File <- tibble(scenario_number = 1:36) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
# print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
# print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
# Make data files for automated data input/output to Hugin
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", tolower(analysed_APIs)),
c("RQ_FW_Estrogens", "RQ_FW_Painkillers","RQ_FW_AllAPI"))
All_RQ_Boolean_Nodes <- c("PRQ_GT_Estrogens", "PRQ_GT_Painkillers", "PRQ_GT_AllAPI")
Hugin_Data_File <- tibble(scenario_number = 1:36) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estriol = "estriol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
# print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
# print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
Temp_Bool_Name
# Make data files for automated data input/output to Hugin
All_RQ_Interval_Nodes <- append(paste0("API_RQ_FW_", tolower(analysed_APIs)),
c("RQ_FW_Estrogens", "RQ_FW_Painkillers","RQ_FW_AllAPI"))
All_RQ_Boolean_Nodes <- c("PRQ_GT_Estrogens", "PRQ_GT_Painkillers", "PRQ_GT_AllAPI")
Hugin_Data_File <- tibble(scenario_number = 1:36) %>%
# Add API node value presets
add_column(API_light_ethinylestradiol = "ethinylestradiol",
API_light_estradiol = "estradiol",
API_medium_diclofenac = "diclofenac",
API_medium_ciprofloxacin = "ciprofloxacin",
API_heavy_paracetamol = "paracetamol",
API_heavy_ibuprofen = "ibuprofen")
# Add columns to monitor RQ intervals for each API and Sum Node
for (v in 1:length(All_RQ_Interval_Nodes)) {
Temp_API_Name <- All_RQ_Interval_Nodes[v]
# print(Temp_API_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_API_Name}", "=0-1)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1-10)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10-100)") := NA,
!! glue("P(", "{Temp_API_Name}", "=100-1000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=1000-10000)") := NA,
!! glue("P(", "{Temp_API_Name}", "=10000-inf)") := NA)
}
# Likewise for each combined probability node
for (v in 1:length(All_RQ_Boolean_Nodes)) {
Temp_Bool_Name <- All_RQ_Boolean_Nodes[v]
# print(Temp_Bool_Name)
Hugin_Data_File <- Hugin_Data_File %>%
add_column(!! glue("P(", "{Temp_Bool_Name}", "=true)") := NA,
!! glue("P(", "{Temp_Bool_Name}", "=false)") := NA)
}
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
write_csv(x = Hugin_Data_File, file = "Data/Hugin/R_to_Hugin_datafile.csv", na = "")
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE)
View(Hugin_Data_Output)
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 6:73,
names_to = "Risk_Bin_String",
values_to = "Probability")
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 6:67,
names_to = "Risk_Bin_String",
values_to = "Probability")
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 2:67,
names_to = "Risk_Bin_String",
values_to = "Probability")
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability")
View(Hugin_Data_Output_Tall)
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# Note the threshold used
Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# Note the threshold used
Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# Note the threshold used
Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("Estradiol", "Ethinylestradiol", "Ciprofloxacin",
"Diclofenac", "Ibuprofen", "Paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability")
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("estradiol", "ethinylestradiol", "Cciprofloxacin",
"diclofenac", "ibuprofen", "paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("estradiol", "ethinylestradiol", "Ciprofloxacin",
"diclofenac", "ibuprofen", "paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE) %>%
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("estradiol", "ethinylestradiol", "ciprofloxacin",
"diclofenac", "ibuprofen", "paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE)
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("estradiol", "ethinylestradiol", "ciprofloxacin",
"diclofenac", "ibuprofen", "paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String) %>%
relocate(Probability, .after = last_col())
# When was the dataset last updated?
Hugin_Output_Last_Updated <- file.info("Data/Hugin/Hugin_to_R_datafile.csv")$mtime
# Hugin has done its thing, so let's import the data
Hugin_Data_Output <- read_csv(file = "Data/Hugin/Hugin_to_R_datafile.csv",
show_col_types = FALSE)
# # Note the threshold used
# Hugin_Threshhold <-  unique(Hugin_Data_Output$PRQ_threshold)
# Hugin's many output interval columns need pivoting to tall data
Hugin_Data_Output_Tall <- Hugin_Data_Output %>%
pivot_longer(cols = 8:67,
names_to = "Risk_Bin_String",
values_to = "Probability") %>%
mutate(Risk_Type = str_remove(Risk_Bin_String, pattern = "_[^_]+$") %>%
str_remove(pattern = "P\\("),
API_Name = str_extract(Risk_Bin_String, pattern = "(?<=\\_)([[:alpha:]]*?)(?=\\=)"),
Risk_Bin = str_extract(Risk_Bin_String, pattern = "(?<==).[a-zA-Z0-9-]+"),
API_Name = fct_relevel(API_Name, c("estradiol", "ethinylestradiol", "ciprofloxacin",
"diclofenac", "ibuprofen", "paracetamol",
"Estrogens", "Painkillers", "AllAPI"))) %>%
select(-Risk_Bin_String, -c(2:7)) %>%
relocate(Probability, .after = last_col())
# When was the dataset last updated?
Hugin_Output_Last_Updated <- file.info("Data/Hugin/Hugin_to_R_datafile.csv")$mtime
# Make a map of Norway in 2020 and 2050 (main/current scenario), with
# - most probable RQ interval displayed as a chloropleth
# - RQ distribution for Viken and Nordland, plus total, as pie charts
# - an accompanying table of extra data?
Norway_county_map_RQ_data <- Norway_county_map_names %>%
left_join(Hugin_Data_Output_Tall, by = c("County_Name" = "master_county")) %>%
filter(master_pop_scenario == "Main", master_WWT_scenario == "Current", API_Name == "AllAPI",
Risk_Bin %notin% c("true", "false")) %>%
group_by(County_Name, master_year) %>%
# What's the most probable bin?
filter(Probability == max(Probability)) %>%
# Give selected regions thicker lines so they stand out
mutate(county_linewidth = case_when(County_Name %in% c("Viken", "Nordland") ~ 2,
TRUE ~ 1))
# label scenarios
scenario_number_labels = tibble(county = "Rural", "Semi-Urban", "Urban")
View(scenario_number_labels)
# label scenarios
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban"))
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(WWT_Scenario = c("Current", "Secondary or better", "Best (all tertiary)"),
Year_and_Population_Growth = c("2020 & 0", "2050 & Low", "2050 & Main", "2050 & High"))
View(scenario_number_labels)
# This turned out to be hard to do automatically, so I did it by hand.
# source(file = "src/data_exporters/WWT_share_CPT_exporter.R")
source(file = "src/data_processing/R_to_Hugin.R")
# label scenarios
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(Year_and_Population_Growth = c("2020 & 0", "2050 & Low", "2050 & Main", "2050 & High"),
WWT_Scenario = c("Current", "Secondary or better", "Best (all tertiary)"))
# This turned out to be hard to do automatically, so I did it by hand.
# source(file = "src/data_exporters/WWT_share_CPT_exporter.R")
source(file = "src/data_processing/R_to_Hugin.R")
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(Year_and_Population_Growth = c("2020 & None", "2050 & Low", "2050 & Main", "2050 & High"),
WWT_Scenario = c("Current", "Secondary or better", "Best (all tertiary)")) %>%
mutate(Scenario_Number = row_number())
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(Year_and_Population_Growth = c("2020 & None", "2050 & Low", "2050 & Main", "2050 & High"),
WWT_Scenario = c("Current", "Secondary or better", "Best (all tertiary)")) %>%
mutate(Scenario_Number = row_number()) %>%
relocate(Scenario_Number, 1)
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(Year_and_Population_Growth = fct_inorder(c("2020 & None", "2050 & Low",
"2050 & Main", "2050 & High")),
WWT_Scenario = c("Current", "Secondary or better", "Best (all tertiary)")) %>%
mutate(Scenario_Number = row_number()) %>%
relocate(Scenario_Number, 1)
# label scenarios
scenario_number_labels = tibble(county = c("Rural", "Semi-Urban", "Urban")) %>%
crossing(Year_and_Population_Growth = fct_inorder(c("2020 & None", "2050 & Low",
"2050 & Main", "2050 & High")),
WWT_Scenario = fct_inorder(c("Current", "Secondary or better", "Best (all tertiary)"))) %>%
mutate(Scenario_Number = row_number()) %>%
relocate(Scenario_Number, 1)
Hugin_Data_Output_Tall
scenario_number_labels
Hugin_Data_Output_Tall_Labelled <- Hugin_Data_Output_Tall %>%
left_join(scenario_number_labels, by = c(scenario_number = Scenario_Number)) %>%
relocate(county, Year_and_Population_Growth, WWT_Scenario, .after = scenario_number)
Hugin_Data_Output_Tall_Labelled <- Hugin_Data_Output_Tall %>%
left_join(scenario_number_labels, by = c("scenario_number" = "Scenario_Number")) %>%
relocate(county, Year_and_Population_Growth, WWT_Scenario, .after = scenario_number)
View(Hugin_Data_Output_Tall_Labelled)
